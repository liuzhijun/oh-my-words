{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583 ['bk_id', 'bk_parent_id', 'bk_level', 'bk_order', 'bk_name', 'bk_item_num', 'bk_direct_item_num', 'bk_author', 'bk_book', 'bk_comment', 'bk_orgnization', 'bk_publisher', 'bk_version', 'bk_flag']\n",
      "70164 ['vc_id', 'vc_vocabulary', 'vc_phonetic_uk', 'vc_phonetic_us', 'vc_frequency', 'vc_difficulty', 'vc_acknowledge_rate']\n",
      "1044580 ['bv_id', 'bv_book_id', 'bv_voc_id', 'bv_flag', 'bv_tag', 'bv_order']\n"
     ]
    }
   ],
   "source": [
    "book = pd.read_csv('DictionaryData/book.csv', sep=\">\")\n",
    "word = pd.read_csv('DictionaryData/word.csv', sep=\">\")\n",
    "relation_book_word = pd.read_csv('DictionaryData/relation_book_word.csv', sep=\">\")\n",
    "print(len(book), list(book.columns))\n",
    "print(len(word), list(word.columns))\n",
    "print(len(relation_book_word), list(relation_book_word.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BookSeries(bk_level=1, bk_order=6.0, bk_name='大学英语四级词汇', bk_item_num=74062, bk_direct_item_num=0, bk_author='', bk_book='', bk_comment='', bk_organization='', bk_publisher='', bk_version='', bk_flag='', bk_series_id='c9f0f895fb98ab9159f51fd0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from database import operation as op\n",
    "op.BookSeries(**{\n",
    "  'bk_level': 1, 'bk_order': 6.0, 'bk_name': '大学英语四级词汇', \n",
    "  'bk_item_num': 74062, 'bk_direct_item_num': 0, 'bk_author': '',\n",
    "  'bk_book': '', 'bk_comment': '', 'bk_organization': '',\n",
    "  'bk_publisher': '', 'bk_version': '', 'bk_flag': '',\n",
    "  'bk_series_id': 'c9f0f895fb98ab9159f51fd0'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from fastapi import Depends, FastAPI, HTTPException\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from database import SessionLocal, engine, Base\n",
    "\n",
    "Base.metadata.create_all(bind=engine)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Dependency\n",
    "def get_db():\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init database\n",
    "from database import model, schema, operation as op\n",
    "\n",
    "s = SessionLocal()\n",
    "book.fillna(value='', inplace=True)\n",
    "word.fillna(value='', inplace=True)\n",
    "relation_book_word.fillna(value='', inplace=True)\n",
    "for i in book.to_dict(orient='records'):\n",
    "    i['bk_organization'] = i['bk_orgnization']\n",
    "    del i['bk_orgnization']\n",
    "    if i['bk_parent_id'] == '0':\n",
    "        del i['bk_parent_id']\n",
    "        i['bk_series_id'] = i['bk_id']\n",
    "        del i['bk_id']\n",
    "        s.add(schema.BookSeries(**i))\n",
    "    else:\n",
    "        del i['bk_parent_id']\n",
    "        s.add(schema.Book(**i))\n",
    "s.commit()\n",
    "for i in word.to_dict(orient='records'):\n",
    "    s.add(schema.Word(**i))\n",
    "s.commit()\n",
    "for i in relation_book_word.to_dict(orient='records'):\n",
    "    s.add(schema.Unit(**i))\n",
    "s.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rel_set = set(relation_book_word['bv_book_id'])\n",
    "# for i in list(relation_book_word['bv_book_id']):\n",
    "#     if i in rel_set:\n",
    "#         rel_set.remove(i)\n",
    "#     else:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bv_id': '58450c828958a37d5c10f763',\n",
       "  'bv_book_id': 'd645920e395fedad7bbbed0e',\n",
       "  'bv_voc_id': '57067b9ca172044907c615d7',\n",
       "  'bv_flag': 4,\n",
       "  'bv_tag': 'Unit 1',\n",
       "  'bv_order': 1},\n",
       " {'bv_id': '58450c828958a37d5c10f764',\n",
       "  'bv_book_id': 'd645920e395fedad7bbbed0e',\n",
       "  'bv_voc_id': '57067c3ca172044907c64fe2',\n",
       "  'bv_flag': 1,\n",
       "  'bv_tag': 'Unit 1',\n",
       "  'bv_order': 2}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_book_word.to_dict(orient='records')[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bk_id': 'c9f0f895fb98ab9159f51fd0',\n",
       "  'bk_parent_id': '0',\n",
       "  'bk_level': 1,\n",
       "  'bk_order': 6.0,\n",
       "  'bk_name': '大学英语四级词汇',\n",
       "  'bk_item_num': 74062,\n",
       "  'bk_direct_item_num': 0,\n",
       "  'bk_author': '',\n",
       "  'bk_book': '',\n",
       "  'bk_comment': '',\n",
       "  'bk_orgnization': '',\n",
       "  'bk_publisher': '',\n",
       "  'bk_version': '',\n",
       "  'bk_flag': ''},\n",
       " {'bk_id': '45c48cce2e2d7fbdea1afc51',\n",
       "  'bk_parent_id': '0',\n",
       "  'bk_level': 1,\n",
       "  'bk_order': 0.0,\n",
       "  'bk_name': '小学英语',\n",
       "  'bk_item_num': 15233,\n",
       "  'bk_direct_item_num': 0,\n",
       "  'bk_author': '',\n",
       "  'bk_book': '',\n",
       "  'bk_comment': '',\n",
       "  'bk_orgnization': '',\n",
       "  'bk_publisher': '',\n",
       "  'bk_version': '',\n",
       "  'bk_flag': ''}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.to_dict(orient='records')[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book.to_sql(name='raw_book', con=engine)\n",
    "# word.to_sql(name='raw_word', con=engine)\n",
    "# relation_book_word.to_sql(name='raw_relation_book_word', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book.to_excel('DictionaryData/book.xlsx', index=False)\n",
    "# word.to_excel('DictionaryData/word.xlsx', index=False)\n",
    "# relation_book_word.to_excel('DictionaryData/relation_book_word.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['vc_id', 'vc_vocabulary', 'vc_phonetic_uk', 'vc_phonetic_us',\n",
       "       'vc_frequency', 'vc_difficulty', 'vc_acknowledge_rate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "886"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "book_name = \"雅思词汇词以类记\"\n",
    "book_id = book[book['bk_name'] == book_name]['bk_id'].values[0]\n",
    "word_batch = []\n",
    "relations_in_book = relation_book_word[relation_book_word['bv_book_id'] == book_id]\n",
    "for i in range(0, len(relations_in_book), batch_size):\n",
    "    relation = relations_in_book[i:i+batch_size]\n",
    "    batch = relation.join(word.set_index('vc_id'), on='bv_voc_id')\n",
    "    # batch = word[word['vc_id'] == relation['bv_voc_id'].values[0]]\n",
    "    batch = batch[['vc_vocabulary', 'vc_phonetic_uk', 'vc_phonetic_us', \n",
    "                   'vc_frequency', 'vc_difficulty', 'vc_acknowledge_rate']]\n",
    "    word_batch.append(batch)\n",
    "len(word_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vc_vocabulary</th>\n",
       "      <th>vc_phonetic_uk</th>\n",
       "      <th>vc_phonetic_us</th>\n",
       "      <th>vc_frequency</th>\n",
       "      <th>vc_difficulty</th>\n",
       "      <th>vc_acknowledge_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>386204</th>\n",
       "      <td>arid</td>\n",
       "      <td>[ˈærɪd]</td>\n",
       "      <td>[ˈærɪd]</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>9</td>\n",
       "      <td>0.286239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386205</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>[ˈætməsfɪə]</td>\n",
       "      <td>[ˈætməsfɪɚ]</td>\n",
       "      <td>0.619064</td>\n",
       "      <td>3</td>\n",
       "      <td>0.746754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386206</th>\n",
       "      <td>balmy</td>\n",
       "      <td>[ˈbɑ:mɪ]</td>\n",
       "      <td>[ˈbɑmi]</td>\n",
       "      <td>0.330187</td>\n",
       "      <td>7</td>\n",
       "      <td>0.265653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386207</th>\n",
       "      <td>barometer</td>\n",
       "      <td>[bəˈrɒmɪtə]</td>\n",
       "      <td>[bəˈrɑmɪtɚ]</td>\n",
       "      <td>0.345525</td>\n",
       "      <td>6</td>\n",
       "      <td>0.233177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386208</th>\n",
       "      <td>blast</td>\n",
       "      <td>[blɑ:st]</td>\n",
       "      <td>[blæst]</td>\n",
       "      <td>0.667218</td>\n",
       "      <td>7</td>\n",
       "      <td>0.359326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       vc_vocabulary vc_phonetic_uk vc_phonetic_us  vc_frequency   \n",
       "386204          arid        [ˈærɪd]        [ˈærɪd]      0.301901  \\\n",
       "386205    atmosphere    [ˈætməsfɪə]    [ˈætməsfɪɚ]      0.619064   \n",
       "386206         balmy       [ˈbɑ:mɪ]        [ˈbɑmi]      0.330187   \n",
       "386207     barometer    [bəˈrɒmɪtə]    [bəˈrɑmɪtɚ]      0.345525   \n",
       "386208         blast       [blɑ:st]        [blæst]      0.667218   \n",
       "\n",
       "        vc_difficulty  vc_acknowledge_rate  \n",
       "386204              9             0.286239  \n",
       "386205              3             0.746754  \n",
       "386206              7             0.265653  \n",
       "386207              6             0.233177  \n",
       "386208              7             0.359326  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import defaultdict, deque\n",
    "from typing import Any, List, Mapping, Optional, Set, Dict, Tuple\n",
    "\n",
    "from langchain import GoogleSearchAPIWrapper\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import cmd\n",
    "import random\n",
    "import sys\n",
    "import traceback\n",
    "import ast\n",
    "from loguru import logger\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import click\n",
    "import tiktoken\n",
    "from langchain.agents import AgentOutputParser\n",
    "from langchain.output_parsers import (\n",
    "    PydanticOutputParser,\n",
    "    OutputFixingParser,\n",
    "    RetryOutputParser,\n",
    ")\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, CombinedMemory, ConversationSummaryMemory\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.callbacks.file import FileCallbackHandler\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.tools import BaseTool, StructuredTool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS\n",
    "\n",
    "\n",
    "from LLM import OpenAIChat\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "log_id = 0\n",
    "\n",
    "def create_chat_message(role: str, content: str):\n",
    "    \"\"\"\n",
    "    Create a chat message with the given role and content.\n",
    "\n",
    "    Args:\n",
    "    role (str): The role of the message sender, e.g., \"system\", \"user\", or \"assistant\".\n",
    "    content (str): The content of the message.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the role and content of the message.\n",
    "    \"\"\"\n",
    "    return {\"role\": role, \"content\": content}\n",
    "\n",
    "llm = OpenAIChat(model_name=\"gpt-3.5-turbo\" , temperature=0.5)\n",
    "\n",
    "# os.environ[\"SERPER_API_KEY\"] = os.getenv(\"SERPER_API_KEY\")\n",
    "os.environ[\"SERPER_API_KEY\"] = \"ab74e952491d09c9007e0fa14c28da535dbec672\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mplease write a story at least 5 sentences long, using the words [arid, atmosphere, balmy, barometer, blast].\n",
      "The word should be in the same order as the words in the prompt.\n",
      "The word should be surrounded by square brackets.\n",
      "Please response in the following format.\n",
      "English: ...[word1]...[word2]...\n",
      "Chinese: ...[单词1]...[单词2]..\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "English: The [arid] desert landscape was unrelenting in its heat, with the [atmosphere] feeling like a suffocating blanket. But as night fell, the temperature dropped and a [balmy] breeze swept through, providing some relief. Checking the [barometer], it seemed that a storm was brewing, and sure enough, a sudden [blast] of wind and rain hit, drenching everything in its path. Despite the chaos, the desert was grateful for the much-needed moisture. \n",
      "\n",
      "Chinese: [干旱的]沙漠景观的热量无情，[气氛]让人感到窒息。但夜幕降临时，温度下降，[温暖的]微风吹过，提供了一些缓解。检查了[气压计]，似乎有一场风暴正在酝酿，果然，一阵突然的[爆炸]风雨袭来，淋湿了一切。尽管混乱，但沙漠仍然感激这场急需的降雨。\n"
     ]
    }
   ],
   "source": [
    "TEMPLATE = \"\"\"\\\n",
    "please write a story at least 5 sentences long, using the words [{words}].\n",
    "The word should be in the same order as the words in the prompt.\n",
    "The word should be surrounded by square brackets.\n",
    "Please response in the following format.\n",
    "English: ...[word1]...[word2]...\n",
    "Chinese: ...[单词1]...[单词2]..\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(\n",
    "    template=TEMPLATE,\n",
    "    input_variables=[\"words\"],\n",
    "    partial_variables={\n",
    "    }\n",
    ")\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True, \n",
    ")\n",
    "words = \", \".join(list(word_batch[0][\"vc_vocabulary\"].values))\n",
    "ans = chain.run(words)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath('.'))\n",
    "sys.path.append(os.path.dirname(SCRIPT_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<User linxy59@qq.com>] [<Book 人教版高中英语1 - 必修>, <Book 人教版高中英语2 - 必修>, <Book 人教版高中英语3 - 必修>, <Book 人教版高中英语4 - 必修>, <Book 人教版高中英语5 - 必修>, <Book 人教版高中英语6 - 选修>, <Book 人教版高中英语8 - 选修>, <Book 人教版高中英语9 - 选修>, <Book 人教版高中英语10 - 选修>, <Book 新概念英语第一册（新版）>, <Book 朗文3000常用交流词汇>, <Book 人教版高中英语11 - 选修>, <Book 新概念英语第二册（新版）>, <Book 新概念英语第三册（新版）>, <Book 新概念英语第四册（新版）>, <Book TOEFL托福45天突破版>, <Book 星火四级词汇周计划>, <Book GRE词汇精选（NEW）>, <Book 六级词汇乱序版>, <Book 专八词汇乱序版>, <Book 雅思词汇加强版>, <Book BEC中级词汇精选>, <Book 词汇进阶 BASIC>, <Book 词汇进阶 6000>, <Book BEC高级词汇精选>, <Book GMAT词汇红宝书>, <Book 词汇进阶 12000>, <Book BEC初级词汇精选>, <Book 雅思词汇便携版>, <Book 星火四级巧记速记乱序版>, <Book 星火全新专8词汇周计划>, <Book 专八词汇正序版>, <Book 王陆807雅思词汇写作第2版>, <Book BEC词汇乱序版>, <Book 专八词汇便携版>, <Book 四级词汇正序版>, <Book 词汇进阶 23000>, <Book 王陆807雅思词汇阅读第2版>, <Book 六级词汇便携版>, <Book 英语四、六级考试词汇必备>, <Book 淘金4级词汇周计划乱序版>, <Book 新概念英语青少版入门级A>, <Book 新概念英语青少版入门级B>, <Book 淘金4级词汇连环记乱序版>, <Book 思思大王六级单词白金版>, <Book 思思大王四级单词白金版>, <Book 全新版大学英语1 (第二版)>, <Book 全新版大学英语4 (第二版)>, <Book 思思大王高考单词一笑而过>, <Book 星火新课标高考英语随身记>, <Book 大学四级考试词汇黑白记忆>, <Book 全新版大学英语2 (第二版)>, <Book 全新版大学英语3 (第二版)>, <Book 高考历年真题核心高频688>, <Book 颠覆高考词汇快速记忆图谱>, <Book 新概念英语青少版3A>, <Book 新概念英语青少版3B>, <Book 新概念英语青少版4A>, <Book 思思大王高考单词白金版>, <Book TOEFL托福词汇正序版>, <Book TOEFL托福词汇乱序版>, <Book TOEFLIBT 词以类记>, <Book GRE词汇精选乱序版>, <Book GRE再要你命3000>, <Book 新概念英语青少版5B>, <Book 新概念英语青少版5A>, <Book TOEIC 990分核心词汇>, <Book 突破高考大纲词汇>, <Book 维克多新高中英语词汇>, <Book 新概念英语青少版4B>, <Book 高中英语词汇便携版>, <Book 高考英语词汇手册>, <Book 托业词汇红宝书>, <Book Academic Voc List (SM2)>, <Book 星火全新专4词汇必背乱序>, <Book 无老师7天搞定托福单词>, <Book 牛津高中英语（模块一）>, <Book 新托福iBT词汇分类突破>, <Book 星火译林版高中英语同步词汇>, <Book 牛津高中英语（模块二）>, <Book 牛津高中英语（模块三）>, <Book 牛津高中英语（模块四）>, <Book 牛津高中英语（模块五）>, <Book 牛津高中英语（模块六）>, <Book 牛津高中英语（模块七）>, <Book 牛津高中英语（模块八）>, <Book 牛津高中英语（模块九）>, <Book 牛津高中英语（模块十）>, <Book 牛津高中英语（模块十一）>, <Book 570个单词轻松征服托福>, <Book 美国当代语料库 5000>, <Book 美国当代语料库 10000>, <Book 美国当代语料库 15000>, <Book 美国当代语料库 20200>, <Book 仁爱版初中英语七年级上册>, <Book 仁爱版初中英语七年级下册>, <Book 仁爱版初中英语八年级上册>, <Book 仁爱版初中英语八年级下册>, <Book 仁爱版初中英语九年级上册>, <Book 仁爱版初中英语九年级下册>, <Book 新视野大学英语读写教程1>, <Book 新概念英语青少版1A>, <Book 新概念英语青少版1B>, <Book 新概念英语青少版2B>, <Book 新概念英语青少版2A>, <Book 新标准大学英语1>, <Book 新标准大学英语2>, <Book 新标准大学英语3>, <Book 新标准大学英语4>, <Book ACT必备核心词汇>, <Book 新视野大学英语读写教程2>, <Book 新视野大学英语读写教程3>, <Book 新视野大学英语读写教程4>, <Book The Oxford 3000 - English>, <Book The Oxford 3000 - American>, <Book 外研社高中英语 必修1>, <Book 外研社高中英语 必修2>, <Book 外研社高中英语 必修3>, <Book 外研社高中英语 必修4>, <Book 外研社高中英语 必修5>, <Book 北师大版高中英语必修模块2>, <Book 北师大版高中英语必修模块3>, <Book 北师大版高中英语必修模块4>, <Book 北师大版高中英语必修模块5>, <Book 人教版初中英语九年级全册>, <Book 人教版初中英语八年级下册>, <Book 星火最新高考词汇必背乱序版>, <Book 人教版初中英语七年级下册>, <Book 人教版初中英语七年级上册>, <Book 人教版初中英语八年级上册>, <Book 华研专八词汇突破13000新题型>, <Book 四级词汇便携版>, <Book 华研六级词汇念念不忘3便携本>, <Book 华研六级词汇念念不忘1基础本>, <Book 华研六级词汇念念不忘2阅读本>, <Book 高考英语分频速记宝典>, <Book Word It! 优先顺序高考单词>, <Book 托福词组必备>, <Book 2016高考英语词汇上海卷>, <Book 全国等级考试词汇必备第一级>, <Book 全国等级考试词汇必备第二级>, <Book 全国等级考试词汇必备第三级>, <Book GMAT词汇词以类记>, <Book 英语（一）自学教程>, <Book 英语（二）自学教程>, <Book 新英语多功能词典>, <Book 考博英语词汇便携版>, <Book 星火高考英语词汇周计划>, <Book 突破英文基础词汇>, <Book 突破英文词汇5000>, <Book 突破英文词汇10000>, <Book 上海市初中英语教学基本要求>, <Book MBA词汇红宝书>, <Book 突破英文词汇22000>, <Book 全国等级考试词汇必备第五级>, <Book 全国等级考试词汇必备第四级>, <Book GRE高分必备短语搭配>, <Book 沪教版牛津英语七年级下册>, <Book 沪教版牛津英语八年级上册>, <Book 沪教版牛津英语八年级下册>, <Book 沪教版牛津英语九年级上册>, <Book 沪教版牛津英语九年级下册>, <Book GMAT词汇精选>, <Book 初中考点精练英语词汇（中考）>, <Book 译林版牛津英语七年级上册>, <Book 译林版牛津英语七年级下册>, <Book 译林版牛津英语八年级下册>, <Book 译林版牛津英语九年级上册>, <Book 译林版牛津英语九年级下册>, <Book 鲁教版五四学制六年级上册>, <Book 鲁教版五四学制六年级下册>, <Book 鲁教版五四学制七年级上册>, <Book 鲁教版五四学制七年级下册>, <Book 鲁教版五四学制八年级上册>, <Book 鲁教版五四学制八年级下册>, <Book 鲁教版五四学制九年级全册>, <Book 北师大版高中英语选修模块6>, <Book 北师大版高中英语选修模块7>, <Book 北师大版高中英语选修模块8>, <Book 北师大版高中英语选修模块9>, <Book 北师大版高中英语选修模块10>, <Book 北师大版高中英语选修模块11>, <Book 星火艾锋四级词汇周计划>, <Book 2016恋练有词考研英语词汇>, <Book 大学英语四六级词组必备>, <Book 朗文国际英语教程(Side by Side)第1册>, <Book 朗文国际英语教程(Side by Side)第2册>, <Book 2016同等学力人员考试词汇>, <Book 抄你老师的英语单词笔记>, <Book 5·3中考英语核心词汇>, <Book 六级词汇分频精讲速记>, <Book 朗文国际英语教程(Side by Side)第3册>, <Book 朗文国际英语教程(Side by Side)第4册>, <Book 剑桥国际英语教程第2册>, <Book 剑桥国际英语教程第1册>, <Book 剑桥国际英语教程入门级>, <Book 新世纪英语专业综合教程3>, <Book 新世纪英语专业综合教程4>, <Book 剑桥国际英语教程第3册>, <Book 专四词汇正序版>, <Book 星火全新专4词汇周计划>, <Book 专四词汇乱序版>, <Book 小学英语升学夺冠知识大集结>, <Book 沪教版牛津英语七年级上册>, <Book 2016雅思词汇乱序版>, <Book 雅思词汇词以类记>, <Book 2014星火六级词汇周计划>, <Book 2017陈正康考研分频第2版>, <Book 2018陈正康考研分频第3版>, <Book PETS全国等级考试教材第三级>, <Book 2015考研英语词汇红宝书>, <Book 2013雅思词汇乱序版>, <Book 2015图解速记高中词汇乱序版>, <Book 2014考研词汇便携版>, <Book 2015考研词汇乱序版>, <Book 2017何凯文考研英语必考词汇>, <Book 2010星火四级词汇必背乱序版>, <Book 2017考研大纲词汇（黄皮书）>, <Book 2014考研英语词汇红宝书>, <Book 2013版四级词汇便携版>, <Book 冀教版三年级起点七年级上册>, <Book 冀教版三年级起点七年级下册>, <Book 冀教版三年级起点八年级上册>, <Book 冀教版三年级起点八年级下册>, <Book 冀教版三年级起点九年级全册>, <Book 2017图解速记高中短语与句型>, <Book 中科院研究生硕士博士词汇速记>, <Book 2007人教版初中八年级下册>, <Book 2007人教版初中九年级>, <Book 2016图解速记高中短语与句型>, <Book 2016思思大王六级一笑而过>, <Book 2017恋练有词考研词组背多分>, <Book 新世纪英语专业综合教程5>, <Book 新世纪英语专业综合教程6>, <Book 四级词汇分频精讲速记>, <Book 2017思思大王六级一笑而过>, <Book 专四词汇突破8000新题型>, <Book 思思大王四级单词一笑而过>, <Book 2017图解速记高中词汇正序版>, <Book 星火全新专8词汇必背>, <Book SAT巴朗词表乱序版>, <Book 2018恋练有词考研英语词汇>, <Book 2018恋练有词考研高分必备锦囊>, <Book 星火全新英语六级词汇必背>, <Book 六级词汇词以类记>, <Book 星火新题型六级词汇周计划>, <Book 星火六级巧记速记乱序版>, <Book 考拉进阶4级词汇周计划>, <Book 新世纪英语专业综合教程1>, <Book 新世纪英语专业综合教程2>, <Book 高考英语分频速记宝典便携版>, <Book 淘金6级词汇周计划乱序版>, <Book TOEFL核心词汇21天突破>, <Book 2016GRE核心词汇考法精析第2版>, <Book 陈正康七天突破必考词组第二版>, <Book 中学英语词汇速记速诀（高考）>, <Book 雅思真词汇（第5版）>, <Book 王陆807雅思词汇口语第2版>, <Book 2017考研英语词汇红宝书>, <Book 2018考研英语词汇红宝书>, <Book 顾家北手把手教你雅思词伙>, <Book 王陆807雅思词汇听力第2版>, <Book 外研社初中英语七年级上册>, <Book 外研社初中英语七年级下册>, <Book 外研社初中英语八年级上册>, <Book 外研社初中英语八年级下册>, <Book 外研社初中英语九年级上册>, <Book 外研社初中英语九年级下册>, <Book 初中英语词汇乱序版>, <Book 新世纪大学英语1（第二版）>, <Book 新世纪大学英语2（第二版）>, <Book 新世纪大学英语4（第二版）>, <Book 2014星火四级词汇必背乱序版>, <Book 淘金式巧攻大学英语四级词汇>, <Book 星火词网六级新大纲>, <Book 2015星火六级词汇周计划>, <Book 大学生英语竞赛C类本科生核心词汇2000>, <Book 华研考研英语二念念不忘基础本>, <Book 华研考研英语二念念不忘便携本>, <Book 2017GRE核心词汇第2版（便携版）>, <Book 专升本英语词汇40天一本通>, <Book 王冬梅雅思词汇过目不忘>, <Book 思思大王考研一笑而过第2版>, <Book 华研考研英语二念念不忘阅读本>, <Book 星火式专业8级词汇巧记速记>, <Book 四级词汇 词以类记>, <Book 四六级常考核心词组1500新题型>, <Book 十天搞定四级词汇便携版>, <Book 四级高频词汇2000>, <Book 念念不忘英语四级词汇>, <Book 英语应用能力考试AB级词汇必背>, <Book 热词红宝书 第1版>, <Book 大学英语精读1（第三版）>, <Book 大学英语精读2（第三版）>, <Book 雅思词组必备>, <Book 雅思分级词汇21天进阶>, <Book 恋练有词四级词汇识记与应用大全>, <Book 大学英语精读3（第三版）>, <Book 译林版牛津英语八年级上册>, <Book SAT词汇乱序版>, <Book 老蒋带你8天搞定英二必考词汇>, <Book 2018高中英语巧记3500词>, <Book 新世纪大学英语3（第二版）>, <Book 北师大版高中英语必修模块1>, <Book 研词考研英语词汇大全解>, <Book 全国成人本科词汇周计划>, <Book 北京成人本科词汇必备>, <Book 2017图解速记高中词汇乱序版>, <Book 外研社高中英语 选修6>, <Book 外研社高中英语 选修7>, <Book 外研社高中英语 选修8>, <Book 外研社高中英语 选修9>, <Book 外研社高中英语 选修10>, <Book 外研社高中英语 选修11>, <Book 大学英语精读4（第三版）>, <Book 2020何凯文考研英语必考词汇>, <Book 星火考研英语大纲词汇表>, <Book 星火考研词汇核心突破>, <Book 人教版三年级起点三年级上>, <Book 人教版三年级起点三年级下>, <Book 人教版三年级起点四年级上>, <Book 人教版三年级起点五年级上>, <Book 人教版三年级起点五年级下>, <Book 人教版三年级起点六年级上>, <Book 人教版三年级起点六年级下>, <Book 人教版一年级起点一年级上>, <Book 人教版一年级起点一年级下>, <Book 人教版一年级起点二年级上>, <Book 人教版一年级起点二年级下>, <Book 人教版一年级起点三年级上>, <Book 人教版一年级起点三年级下>, <Book 人教版一年级起点四年级上>, <Book 人教版一年级起点五年级上>, <Book 人教版一年级起点五年级下>, <Book 人教版一年级起点六年级上>, <Book 人教版一年级起点六年级下>, <Book 人教版一年级起点四年级下>, <Book 译林版三年级起点三年级上>, <Book 译林版三年级起点三年级下>, <Book 译林版三年级起点四年级上>, <Book 译林版三年级起点四年级下>, <Book 译林版三年级起点五年级上>, <Book 译林版三年级起点五年级下>, <Book 译林版三年级起点六年级上>, <Book 译林版三年级起点六年级下>, <Book 外研版三年级起点三年级上>, <Book 外研版三年级起点三年级下>, <Book 外研版三年级起点四年级上>, <Book 外研版三年级起点四年级下>, <Book 外研版三年级起点五年级上>, <Book 外研版三年级起点五年级下>, <Book 外研版三年级起点六年级上>, <Book 外研版三年级起点六年级下>, <Book 译林版小学英语一年级上>, <Book 译林版小学英语一年级下>, <Book 教科版EEC三年级起点三年级上>, <Book 教科版EEC三年级起点三年级下>, <Book 教科版EEC三年级起点四年级上>, <Book 教科版EEC三年级起点四年级下>, <Book 教科版EEC三年级起点五年级上>, <Book 教科版EEC三年级起点五年级下>, <Book 教科版EEC三年级起点六年级上>, <Book 教科版EEC三年级起点六年级下>, <Book 北师大版一年级起点一年级下>, <Book 北师大版一年级起点二年级上>, <Book 北师大版一年级起点一年级上>, <Book 北师大版一年级起点二年级下>, <Book 北师大版一年级起点三年级上>, <Book 北师大版一年级起点三年级下>, <Book 北师大版一年级起点四年级上>, <Book 北师大版一年级起点四年级下>, <Book 北师大版一年级起点五年级上>, <Book 北师大版一年级起点五年级下>, <Book 北师大版一年级起点六年级上>, <Book 北师大版一年级起点六年级下>, <Book 北师大版三年级起点三年级上>, <Book 北师大版三年级起点三年级下>, <Book 北师大版三年级起点四年级上>, <Book 北师大版三年级起点四年级下>, <Book 北师大版三年级起点五年级上>, <Book 北师大版三年级起点五年级下>, <Book 北师大版三年级起点六年级上>, <Book 北师大版三年级起点六年级下>, <Book 北京版一年级起点一年级上>, <Book 北京版一年级起点一年级下>, <Book 北京版一年级起点二年级上>, <Book 北京版一年级起点二年级下>, <Book 北京版一年级起点三年级上>, <Book 北京版一年级起点三年级下>, <Book 北京版一年级起点四年级上>, <Book 北京版一年级起点四年级下>, <Book 北京版一年级起点五年级上>, <Book 北京版一年级起点五年级下>, <Book 北京版一年级起点六年级上>, <Book 北京版一年级起点六年级下>, <Book 沪教版三年级起点三年级下>, <Book 沪教版三年级起点四年级上>, <Book 沪教版三年级起点四年级下>, <Book 沪教版三年级起点五年级上>, <Book 沪教版三年级起点五年级下>, <Book 沪教版三年级起点六年级上>, <Book 沪教版三年级起点六年级下>, <Book 英语词组全书（上）>, <Book 北京成人本科1000核心词汇>, <Book 广州版三年级起点三年级上>, <Book 广州版三年级起点三年级下>, <Book 广州版三年级起点四年级上>, <Book 广州版三年级起点四年级下>, <Book 广州版三年级起点五年级上>, <Book 广州版三年级起点五年级下>, <Book 广州版三年级起点六年级上>, <Book 广州版三年级起点六年级下>, <Book 冀教版三年级起点三年级上>, <Book 冀教版三年级起点三年级下>, <Book 冀教版三年级起点四年级上>, <Book 冀教版三年级起点四年级下>, <Book 冀教版三年级起点五年级上>, <Book 冀教版三年级起点五年级下>, <Book 冀教版三年级起点六年级上>, <Book 冀教版三年级起点六年级下>, <Book 湘少版三年级起点三年级上>, <Book 湘少版三年级起点三年级下>, <Book 湘少版三年级起点四年级上>, <Book 湘少版三年级起点四年级下>, <Book 湘少版三年级起点五年级上>, <Book 湘少版三年级起点五年级下>, <Book 湘少版三年级起点六年级上>, <Book 湘少版三年级起点六年级下>, <Book 闽教版三年级起点三年级上>, <Book 闽教版三年级起点三年级下>, <Book 闽教版三年级起点四年级上>, <Book 闽教版三年级起点四年级下>, <Book 闽教版三年级起点五年级上>, <Book 闽教版三年级起点五年级下>, <Book 闽教版三年级起点六年级上>, <Book 闽教版三年级起点六年级下>, <Book 2003人教版三年级起点四年级下>, <Book 2003人教版三年级起点五年级上>, <Book 2003人教版三年级起点五年级下>, <Book 2004人教版三年级起点六年级上>, <Book 2003人教版三年级起点六年级下>, <Book 成人本科英语词汇正序版>, <Book 人教版三年级起点四年级下>, <Book 沪教版三年级起点三年级上>, <Book 科普版三年级起点三年级上>, <Book 科普版三年级起点四年级上>, <Book 科普版三年级起点六年级上>, <Book 广东版三年级起点三年级上>, <Book 广东版三年级起点三年级下>, <Book 广东版三年级起点四年级上>, <Book 陕西版三年级起点三年级上>, <Book 陕西版三年级起点三年级下>, <Book 译林版小学英语二年级上>, <Book 六级词汇正序版>, <Book 科普版三年级起点四年级下>, <Book 科普版三年级起点五年级上>, <Book 科普版三年级起点五年级下>, <Book 科普版三年级起点六年级下>, <Book 广东版三年级起点四年级下>, <Book 广东版三年级起点五年级上>, <Book 广东版三年级起点五年级下>, <Book 广东版三年级起点六年级上>, <Book 广东版三年级起点六年级下>, <Book 陕西版三年级起点四年级上>, <Book 陕西版三年级起点四年级下>, <Book 陕西版三年级起点五年级上>, <Book 陕西版三年级起点五年级下>, <Book 陕西版三年级起点六年级上>, <Book 陕西版三年级起点六年级下>, <Book 译林版小学英语二年级下>, <Book 四级英语新大纲词汇表>, <Book 科普版三年级起点三年级下>, <Book 译林版牛津小学英语1A>, <Book 译林版牛津小学英语1B>, <Book 星火英语考研巧记速记>, <Book 四级词汇乱序版>, <Book 雅思词汇念念不忘乱序版>, <Book 人教版高中英语7 - 选修>, <Book 2016高中英语巧记3500词>, <Book 终极单词(1) - 变身口语达人3000词>, <Book 终极单词(2) - 成为英语学霸3000词>, <Book 终极单词(3) - 畅读英文报刊3000词>, <Book 英语词汇的奥秘（六级版）>, <Book 十天搞定六级词汇便携版>, <Book 终极单词(4) - 英语母语水平3000词>, <Book 新标准大学英语第二版1>, <Book 新标准大学英语第二版2>, <Book 新标准大学英语第二版3>, <Book 新编大学英语1（第二版 ）>, <Book 2019恋练有词考研英语词组>, <Book 2020陈正康考研分频第5版上册>, <Book 2020陈正康考研分频第5版下册>, <Book 2019刘一男考研词汇速记指南>, <Book 2019李剑考研英语词汇真经>, <Book 考研词汇乱序版>, <Book 考研词汇便携版>, <Book 于慧考研真题100篇词汇分册>, <Book 考研英语词汇强词有理>, <Book 考研词汇闪过>, <Book 十天搞定考研词汇便携版>, <Book 郭崇兴考研精读版>, <Book 考研英语词汇抢分速记蓝皮书>, <Book 考研英语大纲词汇5500>, <Book 考研词汇正序版>, <Book 2020考研英语词汇红宝书>, <Book 考研英语（二）老蒋讲词汇>, <Book 2019何凯文考研英语必考词汇>, <Book 2019陈正康考研分频第4版全册>, <Book 2018刘一男考研词汇速记指南>, <Book 2018恋练有词考研词组背多分>, <Book 2018考研英语高分写作>, <Book 2018考研大纲词汇（黄皮书）>, <Book 2018命题人预测8套卷考研英语（一）>, <Book 新编大学英语2（第二版）>, <Book 新编大学英语3（第二版）>, <Book 新编大学英语4（第二版）>, <Book 十天搞定考研词汇乱序版（第2版）>, <Book 星火抗遗忘高考英语3500词>, <Book 2016思思大王考研单词一笑而过>, <Book 2013四级词汇乱序版>, <Book 2013雅思真词汇（第3版）>, <Book 2015雅思真词汇（第4版）>, <Book 恋练有词高考英语词汇上>, <Book 恋练有词高考英语词汇下>, <Book 高中英语词汇正序版>, <Book 高中英语词汇乱序版>, <Book 恋词考研英语7000词>, <Book 考研词汇笔记>, <Book 单词之间考研词汇>, <Book 六级词汇乱序版>, <Book 新标准大学英语第二版4>, <Book 2020恋练有词考研英语词汇>, <Book 考研深度记忆宝典 1948>, <Book 硕士研究生考试英语大纲(一)>, <Book 星火考研（二）高频1283词>, <Book 2019考研高频核心词汇速记>, <Book 阅读真题精讲 · 28分达考研（一）>, <Book 2016考研英语词汇红宝书>, <Book 2019考研英语词汇红宝书>, <Book 2019恋练有词考研英语词汇>, <Book 2019恋练有词考研高分锦囊>, <Book 2018何凯文考研英语必考词汇>, <Book 十天搞定考研词汇乱序版>, <Book 大学英语精读5（第三版）>, <Book 大学英语精读6（第三版）>, <Book 海文考研分级词汇记忆宝典>, <Book 硕士研究生考试英语大纲(二)>, <Book 译林版牛津小学英语2B>, <Book 译林版牛津小学英语2A>, <Book 高级英语1（第三版重排版）>, <Book 高级英语2（第三版重排版）>, <Book 考研深度记忆宝典>, <Book 六级英语新大纲词汇表>, <Book 恋练有词六级词汇高分锦囊>, <Book 恋练有词六级词汇识记与应用>, <Book 雅思词汇胜经>, <Book 牛津上海版七年级上册>, <Book 牛津上海版七年级下册>]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from database.operation import *\n",
    "from memorize import *\n",
    "from database import SessionLocal, engine, Base\n",
    "\n",
    "Base.metadata.create_all(bind=engine)\n",
    "db = SessionLocal()\n",
    "\n",
    "users = get_all_users(db)\n",
    "# book_series_list = get_book_series_list(db)\n",
    "books = get_all_books(db)\n",
    "\n",
    "print(users, books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 创建记忆计划\n",
    "block_create_user_book = gr.Blocks()\n",
    "\n",
    "with block_create_user_book:\n",
    "    batch_size = gr.State(value=10)\n",
    "    \n",
    "    select_user = gr.Dropdown(\n",
    "        [f\"{user.email}  [{user.id}]\" for user in users], label=\"用户\", info=\"\"\n",
    "    )\n",
    "    # select_book_series = gr.Dropdown(\n",
    "    #     book_series_list, label=\"系列\", info=\"选择一个系列\"\n",
    "    # )\n",
    "    select_book = gr.Dropdown(\n",
    "        [f\"{book.bk_name}  [{book.bk_id}]\" for book in books],\n",
    "        # books,\n",
    "        label=\"单词书\", info=\"选择一本单词书\"\n",
    "    )\n",
    "    # def on_select_user(user):\n",
    "    #     select_user.\n",
    "    #     return user\n",
    "    # select_user.change(on_select_user, inputs=[], outputs=[])\n",
    "    batch_size = gr.Number(value=10)\n",
    "    title = gr.TextArea(value='单词书', lines=1)\n",
    "    btn = gr.Button(\"创建记忆计划\")\n",
    "    status = gr.Textbox(\"\", lines=1)\n",
    "    def submit(user, book, title, batch_size):\n",
    "        user_id = user.split(\" [\")[1][:-1]\n",
    "        book_id = book.split(\" [\")[1][:-1]\n",
    "        user_book = create_user_book(db, UserBookCreate(\n",
    "            owner_id=user_id,\n",
    "            book_id=book_id,\n",
    "            title=title,\n",
    "            batch_size=batch_size\n",
    "        ))\n",
    "        if user_book is not None:\n",
    "            return \"success\"\n",
    "        else:\n",
    "            return \"fail\"\n",
    "\n",
    "    btn.click(submit, [select_user, select_book, title, batch_size], [status])\n",
    "\n",
    "block_create_user_book.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user linxy59@qq.com  [443af7e1-5a7d-422f-8bb4-5fea35c44718]\n",
      "user_book 雅思词汇词以类记 | 5个单词一组 [780582d3-c59b-4cee-86cb-15d7eab4a964]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mplease write a story at least 5 sentences long, using the words [['arid', 'atmosphere', 'balmy', 'barometer', 'blast']].\n",
      "The word should be in the same order as the words in the prompt.\n",
      "The word should be surrounded by square brackets.\n",
      "Please response the story in the following format.\n",
      "English: ...[word1]...[word2]...\n",
      "Chinese: ...[单词1]...[单词2]..\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"story\": {\"title\": \"Story\", \"description\": \"the story\", \"type\": \"string\"}, \"translated_story\": {\"title\": \"Translated Story\", \"description\": \"the translated story\", \"type\": \"string\"}}, \"required\": [\"story\", \"translated_story\"]}\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "fb60441c-d372-4359-a723-2aa8d52b10b6\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mplease write a story at least 5 sentences long, using the words [['blizzard', 'breeze', 'chill', 'chilly', 'climate']].\n",
      "The word should be in the same order as the words in the prompt.\n",
      "The word should be surrounded by square brackets.\n",
      "Please response the story in the following format.\n",
      "English: ...[word1]...[word2]...\n",
      "Chinese: ...[单词1]...[单词2]..\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"story\": {\"title\": \"Story\", \"description\": \"the story\", \"type\": \"string\"}, \"translated_story\": {\"title\": \"Translated Story\", \"description\": \"the translated story\", \"type\": \"string\"}}, \"required\": [\"story\", \"translated_story\"]}\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "001ee967-172b-4030-83e2-1fd7bd63aaf2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 选择并批量单词\n",
    "block_select_and_batching_the_words = gr.Blocks()\n",
    "\n",
    "with block_select_and_batching_the_words:\n",
    "    select_user = gr.Dropdown(\n",
    "        [f\"{user.email}  [{user.id}]\" for user in users], label=\"用户\", info=\"\"\n",
    "    )\n",
    "    select_user_book = gr.Dropdown(\n",
    "        [], label=\"记忆计划\", info=\"选择用户的记忆计划\"\n",
    "    )\n",
    "    known_words = gr.CheckboxGroup(\n",
    "        [], label=\"已学会的单词\", info=\"正式记忆前将去除已学会的单词，提高每个批次的新词密度，进而提高效率\"\n",
    "    )\n",
    "    def on_select_user(user):\n",
    "        print('user', user)\n",
    "        if user is None:\n",
    "            return gr.Dropdown.update(choices=[])\n",
    "        new_options =  []\n",
    "        user_id = user.split(\" [\")[1][:-1]\n",
    "        user_book = get_user_books_by_owner_id(db, user_id)\n",
    "        new_options = [f\"{book.title} | {book.batch_size}个单词一组 [{book.id}]\" for book in user_book]\n",
    "        return gr.Dropdown.update(choices=new_options)\n",
    "\n",
    "    def on_select_user_book(user_book):\n",
    "        print('user_book', user_book)\n",
    "        if user_book is None:\n",
    "            return gr.CheckboxGroup.update(choices=[])\n",
    "        new_options = []\n",
    "        user_book_id = user_book.split(\" [\")[1][:-1]\n",
    "        user_book = get_user_book(db, user_book_id)\n",
    "        book_id = user_book.book_id\n",
    "        book = get_book(db, book_id)\n",
    "        if book is None:\n",
    "            return gr.CheckboxGroup.update(choices=new_options)\n",
    "        words = get_words_for_book(db, user_book)\n",
    "        new_options = [f\"{word.vc_vocabulary}\" for word in words]\n",
    "        return gr.CheckboxGroup.update(choices=new_options)\n",
    "    \n",
    "    select_user.select(on_select_user, inputs=[select_user], outputs=[select_user_book])\n",
    "    select_user_book.select(on_select_user_book, inputs=[select_user_book], outputs=[known_words])\n",
    "\n",
    "    btn = gr.Button(\"生成批次\", elem_id=\"btn\", elem_classes=[\"abc\", \"def\"])\n",
    "    status = gr.Textbox(\"\", lines=1)\n",
    "\n",
    "    def submit(user_book, known_words):\n",
    "        user_book_id = user_book.split(\" [\")[1][:-1]\n",
    "        user_book = get_user_book(db, user_book_id)\n",
    "        all_words = get_words_for_book(db, user_book)\n",
    "        unknown_words = []\n",
    "        for w in all_words:\n",
    "            if w.vc_vocabulary not in known_words:\n",
    "                unknown_words.append(w)\n",
    "        track(db, user_book, unknown_words[:10])\n",
    "        return \"success\"\n",
    "\n",
    "    btn.click(submit, [select_user_book, known_words], [status])\n",
    "\n",
    "block_select_and_batching_the_words.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user linxy59@qq.com  [443af7e1-5a7d-422f-8bb4-5fea35c44718]\n",
      "user_book 雅思词汇词以类记 | 5个单词一组 [780582d3-c59b-4cee-86cb-15d7eab4a964]\n",
      "           单词                                     意思   \n",
      "0       blast  n.一阵(风),一股(气流),爆炸,冲击波vt.爆炸,,毁灭,使枯萎,损害  \\\n",
      "1  atmosphere                             n.大气,空气,气氛   \n",
      "2   barometer                                  n.气压计   \n",
      "3       balmy                adj.芳香的,温和的,止痛的,(空气)温和的   \n",
      "4        arid               adj.干旱的,贫瘠的(土地等),无趣的,沉闷的   \n",
      "\n",
      "                                     id    记忆量  \n",
      "0  409d4f1a-6f1f-49cc-99ea-899afac09391  0 / 2  \n",
      "1  fe659943-bb5e-4dea-9154-0b7336c7dc05  1 / 2  \n",
      "2  8b968f99-923c-4863-b18b-ac14824fb0cc  0 / 2  \n",
      "3  f8923df6-779e-49b7-9fa2-1caa7a8fcceb  0 / 2  \n",
      "4  bc26f24d-fc42-4ee6-9cc2-959dd549ec34  0 / 2  \n",
      "['blast', 'atmosphere', 'barometer', 'balmy', 'arid']\n",
      "The **arid** desert **atmosphere** was suddenly interrupted by a **balmy** **blast** of wind. The **barometer** dropped as sand and debris flew through the air. The travelers huddled together, trying to shield themselves from the onslaught. Finally, as quickly as it had come, the wind died down and the **arid** **atmosphere** returned.\n",
      "沙漠的**干燥** **氛围**突然被一阵**温暖的** **爆炸**风吹袭。**气压计**下降，沙子和碎片在空中飞舞。旅行者们聚在一起，试图保护自己免受袭击。最终，风停了，**干燥**的**氛围**回归了。\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "# 选择并批量单词\n",
    "block_remember = gr.Blocks()\n",
    "\n",
    "with block_remember:\n",
    "    select_user = gr.Dropdown(\n",
    "        [f\"{user.email}  [{user.id}]\" for user in users], label=\"用户\", info=\"\"\n",
    "    )\n",
    "    select_user_book = gr.Dropdown(\n",
    "        [], label=\"记忆计划\", info=\"请选择记忆计划\"\n",
    "    )\n",
    "    memorizing_dataframe = gr.Dataframe(\n",
    "        headers=[\"单词\", \"意思\", \"id\", \"记忆量\"],\n",
    "        datatype=[\"str\", \"str\", \"str\", \"str\"],\n",
    "        col_count=(4, \"fixed\"),\n",
    "        wrap=True,\n",
    "    )\n",
    "    batches = gr.State(value=[])\n",
    "    current_batch_index = gr.State(value=-1)\n",
    "    with gr.Row():\n",
    "        # story = gr.HighlightedText([])\n",
    "        # translated_story = gr.HighlightedText([])\n",
    "        # story = gr.Textbox()\n",
    "        # translated_story = gr.Textbox()\n",
    "        story = gr.Markdown()\n",
    "        translated_story = gr.Markdown()\n",
    "    \n",
    "    memorize_action = gr.CheckboxGroup(choices=[], label=\"记住的单词\", info=\"能够复述出意思才算记住\")\n",
    "    with gr.Row():\n",
    "        previous_batch_btn = gr.Button(\"上一批\")\n",
    "        next_batch_btn = gr.Button(\"下一批\")\n",
    "    progress = gr.Slider(0, 1, value=0, step=1, label=\"进度\", info=\"\")\n",
    "\n",
    "    def on_select_user(user):\n",
    "        print('user', user)\n",
    "        if user is None:\n",
    "            return gr.Dropdown.update(choices=[])\n",
    "        new_options =  []\n",
    "        user_id = user.split(\" [\")[1][:-1]\n",
    "        user_book = get_user_books_by_owner_id(db, user_id)\n",
    "        new_options = [f\"{book.title} | {book.batch_size}个单词一组 [{book.id}]\" for book in user_book]\n",
    "        return gr.Dropdown.update(choices=new_options)\n",
    "\n",
    "    def process(story: str):\n",
    "        # return [(i, \"\") for i in story.split(\" \")]\n",
    "        return story.replace(\"[\", \"**\").replace(\"]\", \"**\")\n",
    "    def update_batch(memorizing_batch: UserMemoryBatch):\n",
    "        new_options = []\n",
    "        word_df = []\n",
    "        # print(get_user_memory_batch(db, memorizing_batch.id))\n",
    "        # print(memorizing_batch.id)\n",
    "        # print(get_user_memory_words_by_batch_id(db, memorizing_batch.id))\n",
    "        # print(get_words_by_ids(db, [w.word_id for w in get_user_memory_words_by_batch_id(db, memorizing_batch.id)]))\n",
    "        # words = get_words_in_batch(db, memorizing_batch.id)\n",
    "        # words = get_words_by_ids(db, [w.word_id for w in memorizing_words])\n",
    "        memorizing_words = get_user_memory_words_by_batch_id(db, memorizing_batch.id)\n",
    "        word_to_memorizing = {mw.word_id:mw.id for mw in memorizing_words}\n",
    "        memorizing_to_word = {mw.id:mw.word_id for mw in memorizing_words}\n",
    "        words = get_words_by_ids(db, [w.word_id for w in memorizing_words])\n",
    "        actions = get_actions_at_each_word(db, [w.id for w in memorizing_words])\n",
    "        remember_count = defaultdict(int)\n",
    "        forget_count = defaultdict(int)\n",
    "        for a in actions:\n",
    "            if a.action == \"remember\":\n",
    "                remember_count[memorizing_to_word[a.user_memory_word_id]] += 1\n",
    "            else:\n",
    "                forget_count[memorizing_to_word[a.user_memory_word_id]] += 1\n",
    "        for w in words:\n",
    "            new_options.append(f\"{w.vc_vocabulary}\")\n",
    "            word_df.append([w.vc_vocabulary, w.vc_translation, word_to_memorizing[w.vc_id], f\"{remember_count[w.vc_id]} / {remember_count[w.vc_id] + forget_count[w.vc_id]}\"])\n",
    "        df = pd.DataFrame(word_df, columns=[\"单词\", \"意思\", \"id\", \"记忆量\"])\n",
    "        print(df)\n",
    "        print(new_options)\n",
    "        story = memorizing_batch.story\n",
    "        story = process(story)\n",
    "        print(story)\n",
    "        translated_story = memorizing_batch.translated_story\n",
    "        translated_story = process(translated_story)\n",
    "        print(translated_story)\n",
    "        # df = gr.DataFrame.update(\n",
    "        #     value=df,\n",
    "        #     max_rows=len(df),\n",
    "        # )\n",
    "        return (df, story, translated_story, gr.CheckboxGroup.update(choices=new_options))\n",
    "\n",
    "    def on_select_user_book(user_book: str):\n",
    "        \"\"\"\n",
    "        1. 当前单词\n",
    "        2. 对当前单词的操作\n",
    "        3. 故事\n",
    "        \"\"\"\n",
    "        print('user_book', user_book)\n",
    "        if user_book is None:\n",
    "            return [], gr.CheckboxGroup.update(choices=[])\n",
    "        user_book_id: str = user_book.split(\" [\")[1][:-1]\n",
    "        user_book = get_user_book(db, user_book_id)\n",
    "        batches = get_user_memory_batches_by_user_book_id(db, user_book_id)\n",
    "        batch_id = user_book.memorizing_batch\n",
    "        memorizing_batch = get_user_memory_batch(db, batch_id)\n",
    "        current_batch_index = -1\n",
    "        if memorizing_batch is not None:\n",
    "            for index, b in enumerate(batches):\n",
    "                if b.id == memorizing_batch.id:\n",
    "                    current_batch_index = index\n",
    "                    break\n",
    "        if current_batch_index == -1:\n",
    "            current_batch_index = 0\n",
    "            memorizing_batch = batches[0]\n",
    "            batch_id = memorizing_batch.id\n",
    "            user_book.memorizing_batch = batch_id\n",
    "            update_user_book(db, user_book_id, UserBookUpdate(\n",
    "                owner_id=user_book.owner_id,\n",
    "                book_id=user_book.book_id,\n",
    "                title=user_book.title,\n",
    "                batch_size=user_book.batch_size,\n",
    "                memorizing_batch=batch_id\n",
    "            ))\n",
    "        updates = update_batch(memorizing_batch)\n",
    "        print(len(batches))\n",
    "        return (batches, current_batch_index) + updates + (\n",
    "                gr.Slider.update(\n",
    "                    minimum=1,\n",
    "                    maximum=len(batches),\n",
    "                    value=current_batch_index,\n",
    "                ),)\n",
    "    batch_widget = [memorizing_dataframe, story, translated_story, memorize_action]\n",
    "    select_user.select(on_select_user, inputs=[select_user], outputs=[select_user_book])\n",
    "    select_user_book.select(\n",
    "        on_select_user_book,\n",
    "        inputs=[select_user_book],\n",
    "        outputs=[batches, current_batch_index] + batch_widget + [progress]\n",
    "    )\n",
    "    def submit_batch(batches, current_batch_index):\n",
    "        memorizing_batch = batches[current_batch_index]\n",
    "        updates = update_batch(memorizing_batch)\n",
    "        return updates + (gr.Slider.update(value=current_batch_index+1),)\n",
    "    def previous_batch(batches, current_batch_index):\n",
    "        if current_batch_index <= 0:\n",
    "            current_batch_index = 0\n",
    "        elif current_batch_index > 0:\n",
    "            current_batch_index -= 1\n",
    "        return submit_batch(batches, current_batch_index)\n",
    "    def next_batch(batches: List[UserMemoryBatch], current_batch_index: int, memorizing_dataframe: pd.DataFrame, memorize_action: List[str]):\n",
    "        old_index = current_batch_index\n",
    "        if current_batch_index >= len(batches)-1:\n",
    "            current_batch_index = len(batches)-1\n",
    "        elif current_batch_index < len(batches) - 1:\n",
    "            current_batch_index += 1\n",
    "        if current_batch_index != old_index:\n",
    "            # 下一页之前需要保存记忆进度 \n",
    "            print(\"下一页之前需要保存记忆进度\")\n",
    "            print(memorizing_dataframe)\n",
    "            print(memorize_action)\n",
    "            old_batch = batches[old_index]\n",
    "            actions = []\n",
    "            for i in range(len(memorizing_dataframe)):\n",
    "                row = memorizing_dataframe.iloc[i]\n",
    "                word_i = row[\"单词\"]\n",
    "                memorizing_i = row[\"id\"]\n",
    "                if word_i in memorize_action:\n",
    "                    actions.append((memorizing_i, \"remember\"))\n",
    "                else:\n",
    "                    actions.append((memorizing_i, \"forget\"))\n",
    "            memory_batch_action(db, actions)\n",
    "        return submit_batch(batches, current_batch_index)\n",
    "    previous_batch_btn.click(\n",
    "        previous_batch, \n",
    "        inputs=[batches, current_batch_index],\n",
    "        outputs=batch_widget + [progress]\n",
    "    )\n",
    "    next_batch_btn.click(\n",
    "        next_batch,\n",
    "        inputs=[batches, current_batch_index, memorizing_dataframe, memorize_action],\n",
    "        outputs=batch_widget + [progress]\n",
    "    )\n",
    "\n",
    "block_remember.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.30.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: setuptools in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (66.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lxy/anaconda3/envs/py39/lib/python3.9/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Running on local URL:  http://127.0.0.1:7874\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "os.system('python -m spacy download en_core_web_sm')\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def text_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    html = displacy.render(doc, style=\"dep\", page=True)\n",
    "    html = (\n",
    "        \"<div style='max-width:100%; max-height:360px; overflow:auto'>\"\n",
    "        + html\n",
    "        + \"</div>\"\n",
    "    )\n",
    "    pos_count = {\n",
    "        \"char_count\": len(text),\n",
    "        \"token_count\": 0,\n",
    "    }\n",
    "    pos_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        pos_tokens.extend([(token.text, token.pos_), (\" \", None)])\n",
    "\n",
    "    return pos_tokens, pos_count, html\n",
    "\n",
    "demo = gr.Interface(\n",
    "    text_analysis,\n",
    "    gr.Textbox(placeholder=\"Enter sentence here...\"),\n",
    "    [\"highlight\", \"json\", \"html\"],\n",
    "    examples=[\n",
    "        [\"What a beautiful morning for a walk!\"],\n",
    "        [\"It was the best of times, it was the worst of times.\"],\n",
    "    ],\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
